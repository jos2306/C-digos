{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import asarray\n",
    "import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import GlobalAveragePooling2D,Input, ReLU, Dense, Activation, ZeroPadding2D, Dropout,BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPool2D, Add, MaxPooling2D\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.models import save_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau \n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.resnet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import utils as np_utils\n",
    "import keras.backend as K\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "########################## librerias para el desbalance de datos #######################\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "########################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de un directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/joshua6090/Documentos/DL_AECG_P/anomalias_en_electrocardiogramas_si/kunst/PosibleProp/Papers/spectrogram_ecg_cnn/RP_plt\"\n",
    "dir_list=os.listdir(data_path)\n",
    "print('Tu puta lista de archivos:',dir_list)\n",
    "label=['N','Q','S','V','F']\n",
    "N=[]\n",
    "Q=[]\n",
    "S=[]\n",
    "V=[]\n",
    "F=[]\n",
    "\n",
    "for name_dir in dir_list:\n",
    "    dir_path=data_path + '/' + name_dir\n",
    "    print(dir_path) #### detecta las carpetas de las clases que tenemos\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        image_path=dir_path + '/' + file_name #### tengo todas las dirrecciones de las imagenes\n",
    "        checar= name_dir\n",
    "        if checar == 'N':\n",
    "            N.append(image_path)\n",
    "        elif checar== 'Q':\n",
    "            Q.append(image_path)\n",
    "        elif checar== 'S':\n",
    "            S.append(image_path)\n",
    "        elif checar== 'V':\n",
    "            V.append(image_path)\n",
    "        else:\n",
    "            F.append(image_path)\n",
    "\n",
    "print('Hay {} N'.format(len(N)))\n",
    "print('Hay {} Q'.format(len(Q)))\n",
    "print('Hay {} S'.format(len(S)))\n",
    "print('Hay {} V'.format(len(V)))\n",
    "print('Hay {} F'.format(len(F)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelEncoder \n",
    "plt.rcParams.update({'font.size': 16})\n",
    "#### ramdom no seas wey, solo es eso\n",
    "np.random.shuffle(N)\n",
    "np.random.shuffle(Q)\n",
    "np.random.shuffle(S)\n",
    "np.random.shuffle(V)\n",
    "np.random.shuffle(F)\n",
    "\n",
    "#####################\n",
    "#### split \n",
    "\n",
    "train_N, test_N = np.split(N, [int(len(N)*0.8)])\n",
    "train_Q, test_Q = np.split(Q, [int(len(Q)*0.8)])\n",
    "train_S, test_S = np.split(S, [int(len(S)*0.8)])\n",
    "train_V, test_V = np.split(V, [int(len(V)*0.8)])\n",
    "train_F, test_F = np.split(F, [int(len(F)*0.8)])\n",
    "\n",
    "### DataFrames\n",
    "train_N_df = pd.DataFrame({'image':train_N, 'label':'N'})\n",
    "test_N_df = pd.DataFrame({'image':test_N, 'label':'N'})\n",
    "\n",
    "train_Q_df = pd.DataFrame({'image':train_Q, 'label':'Q'})\n",
    "test_Q_df = pd.DataFrame({'image':test_Q, 'label':'Q'})\n",
    "\n",
    "train_S_df = pd.DataFrame({'image':train_S, 'label':'S'})\n",
    "test_S_df = pd.DataFrame({'image':test_S, 'label':'S'})\n",
    "\n",
    "train_V_df = pd.DataFrame({'image':train_V, 'label':'V'})\n",
    "test_V_df = pd.DataFrame({'image':test_V, 'label':'V'})\n",
    "\n",
    "train_F_df = pd.DataFrame({'image':train_F, 'label':'F'})\n",
    "test_F_df = pd.DataFrame({'image':test_F, 'label':'F'})\n",
    "### Juntar estas madres\n",
    "\n",
    "train_df = pd.concat([train_N_df, train_Q_df, train_S_df , train_V_df, train_F_df])\n",
    "test_df = pd.concat([test_N_df, test_Q_df, test_S_df, test_V_df, test_F_df])\n",
    "#####################\n",
    "#para  como se ve la distribución de los datos\n",
    "todo_N = pd.DataFrame({'image':N, 'label':'N'})\n",
    "todo_Q = pd.DataFrame({'image':Q, 'label':'Q'})\n",
    "todo_S = pd.DataFrame({'image':S, 'label':'S'})\n",
    "todo_V = pd.DataFrame({'image':V, 'label':'V'})\n",
    "todo_F = pd.DataFrame({'image':F, 'label':'F'})\n",
    "todo = pd.concat([todo_N,todo_Q,todo_S,todo_V,todo_F])\n",
    "\n",
    "X=np.array(todo[\"image\"].tolist())\n",
    "X=X.reshape(-1,1)\n",
    "y=np.array(todo[\"label\"].tolist())\n",
    "y.reshape(-1,1)\n",
    "###\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "w2=pd.DataFrame({'image':X.flatten(), 'label':y})\n",
    "X_train_todo,y_test_todo = train_test_split(w2, test_size=0.2)\n",
    "###############\n",
    "#le=LabelEncoder()\n",
    "#y=le.fit_transform(y)\n",
    "##primero a datframe\n",
    "#w=pd.DataFrame({'image':x})\n",
    "#w1=pd.DataFrame({'label':y})\n",
    "### junto\n",
    "#xy=pd.concat([w,w1])\n",
    "#####################\n",
    "### Vamo a visualizar la distribución\n",
    "fig=sns.countplot(x='label',data=todo)\n",
    "plt.xlabel(\"Categorías\")\n",
    "plt.ylabel(\"Número de muestras\")\n",
    "plt.title(\"Distribución de la base de datos antes del RUS\") \n",
    "plt.show(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "# Calcular el conteo de cada categoría\n",
    "categories = todo['label'].value_counts()\n",
    "\n",
    "# Definir los datos para el diagrama de torta\n",
    "labels = categories.index\n",
    "sizes = categories.values\n",
    "explode = (0, 0.1, 0.3, 0.5,0.9)  # No se hará \"explode\" en ninguna categoría\n",
    "\n",
    "# Crear el diagrama de torta y ajustar el tamaño de la fuente de las etiquetas\n",
    "plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "\n",
    "# Configurar el aspecto del gráfico\n",
    "plt.title(\"MIT-BIH\")\n",
    "plt.axis('equal')  # Para que el gráfico sea un círculo en lugar de una elipse\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema de des-balanceamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Primero ocuparemos el método de RandomUnderSampling\n",
    "rus=RandomUnderSampler(sampling_strategy={'N':802,'Q':802,'S':802,'V':802,'F':802})\n",
    "X_train_rus,y_test_rus=rus.fit_resample(X,y)\n",
    "###Para ver el número de records\n",
    "print(sorted(Counter(y_test_rus).items()))\n",
    "\n",
    "###Concatnarlos en un data frame\n",
    "#X_train_rus=np.array(X_train_rus.tolist())\n",
    "w=pd.DataFrame({'image':X_train_rus.flatten(), 'label':y_test_rus})\n",
    "#### como quedo despues del RUS\n",
    "fig=sns.countplot(x='label',data=w)\n",
    "plt.xlabel(\"Categorías\")\n",
    "plt.ylabel(\"Número de muestras\")\n",
    "plt.title(\"Distribución de la base de datos después del RUS\") \n",
    "plt.show(fig)\n",
    "###########################\n",
    "X_train,y_test = train_test_split(w, test_size=0.2)\n",
    "XX_Train=X_train.image\n",
    "yy_Test=y_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular el conteo de cada categoría después del RUS\n",
    "categories_rus = w['label'].value_counts()\n",
    "\n",
    "# Crear el gráfico de pastel\n",
    "plt.pie(categories_rus, labels=categories_rus.index, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "\n",
    "# Configurar el aspecto del gráfico\n",
    "plt.title(\"Distribución de la base de datos después del RUS.\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular el conteo de cada categoría\n",
    "categories = todo['label'].value_counts()\n",
    "\n",
    "# Crear el gráfico de pastel\n",
    "plt.pie(categories, labels=categories.index, autopct='')\n",
    "\n",
    "# Configurar el aspecto del gráfico\n",
    "plt.title(\"MIT-BIH\")\n",
    "plt.axis('equal')  # Para que el gráfico sea un círculo en lugar de una elipse\n",
    "\n",
    "# Calcular el centro del gráfico de pastel\n",
    "x, y = 0, 0\n",
    "\n",
    "# Calcular el radio del gráfico de pastel\n",
    "radius = 0.5\n",
    "\n",
    "# Mostrar los porcentajes fuera del gráfico\n",
    "total = sum(categories)\n",
    "previous_angle = 0\n",
    "\n",
    "for category in categories.index:\n",
    "    count = categories[category]\n",
    "    percentage = count / total * 100\n",
    "\n",
    "    # Calcular el ángulo correspondiente al segmento de cada categoría\n",
    "    angle = 2 * 3.14159 * count / total + previous_angle\n",
    "\n",
    "    # Calcular las coordenadas del punto de anotación\n",
    "    x_anno = x + (radius + 0.1) * 0.5 * np.cos(angle)\n",
    "    y_anno = y + (radius + 0.1) * 0.5 * np.sin(angle)\n",
    "\n",
    "    # Mostrar el porcentaje como anotación\n",
    "    plt.annotate(f'{percentage:.1f}%', (x_anno, y_anno), ha='center', va='center', fontsize=10)\n",
    "\n",
    "    previous_angle = angle\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', header=False, index=False)\n",
    "y_test.to_csv('y_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Empieza lo chido mi prro\n",
    "batch_size = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "#create the ImageDataGenerator object and rescale the images\n",
    "trainGenerator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "testGenerator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "#convert them into a dataset\n",
    "trainDataset = trainGenerator.flow_from_dataframe(\n",
    "  dataframe=X_train_todo,\n",
    "  class_mode='categorical',\n",
    "  x_col=\"image\",\n",
    "  y_col=\"label\",\n",
    "  batch_size=batch_size,\n",
    "  seed=42,\n",
    "  shuffle=True,\n",
    "  target_size=(IMG_HEIGHT,IMG_WIDTH) \n",
    ")\n",
    "\n",
    "\n",
    "testDataset = testGenerator.flow_from_dataframe(\n",
    "  dataframe=y_test_todo,\n",
    "  class_mode='categorical',\n",
    "  x_col=\"image\",\n",
    "  y_col=\"label\",\n",
    "  batch_size=batch_size,\n",
    "  seed=42,\n",
    "  shuffle=False,\n",
    "  target_size=(IMG_HEIGHT,IMG_WIDTH)\n",
    ")\n",
    "completo=trainDataset.filenames\n",
    "nb_train_samples = len(trainDataset.filenames)\n",
    "nb_validation_samples = len(trainDataset.filenames)\n",
    "Hola=testDataset.class_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_SCHEME = \"he_normal\"\n",
    "def res_block(X, filter, stage):\n",
    "      \n",
    "    # Convolutional_block\n",
    "    X_copy = X\n",
    "\n",
    "    f1 , f2, f3 = filter\n",
    "    \n",
    "    # Main Path\n",
    "    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = MaxPool2D((2,2))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n",
    "\n",
    "\n",
    "    # Short path\n",
    "    X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= INIT_SCHEME)(X_copy)\n",
    "    X_copy = MaxPool2D((2,2))(X_copy)\n",
    "    X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n",
    "\n",
    "    # ADD\n",
    "    X = Add()([X, X_copy])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Identity Block 1\n",
    "    X_copy = X\n",
    "    \n",
    "\n",
    "    # Main Path\n",
    "    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n",
    "\n",
    "    # ADD\n",
    "    X = Add()([X, X_copy])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Identity Block 2\n",
    "    X_copy = X\n",
    "\n",
    "\n",
    "    # Main Path\n",
    "    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= INIT_SCHEME)(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n",
    "\n",
    "    # ADD\n",
    "    X = Add()([X, X_copy])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "# Input tensor shape\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# Zero-padding\n",
    "X = ZeroPadding2D((3,3))(X_input)\n",
    "\n",
    "# 1- stage\n",
    "X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= INIT_SCHEME)(X)\n",
    "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3,3), strides= (2,2))(X)\n",
    "\n",
    "# 2- stage\n",
    "X = res_block(X, filter= [64,64,256], stage= 2)\n",
    "\n",
    "# 3- stage\n",
    "X = res_block(X, filter= [128,128,512], stage= 3)\n",
    "\n",
    "# 4- stage\n",
    "X = res_block(X, filter= [256,256,1024], stage= 4)\n",
    "\n",
    "# 5- stage\n",
    "X = res_block(X, filter= [512,512,2048], stage= 5)\n",
    "\n",
    "# Average Pooling\n",
    "X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n",
    "\n",
    "# Final layer\n",
    "X = Flatten()(X)\n",
    "X = Dropout(0.8)(X)\n",
    "X = Dense(5, activation = 'softmax', name = 'Dense_final', kernel_initializer= INIT_SCHEME )(X)\n",
    "\n",
    "# Build model.\n",
    "model = Model(\n",
    "    inputs= X_input, \n",
    "    outputs = X, \n",
    "    name = 'Resnet18'\n",
    "    \n",
    ")\n",
    "\n",
    "# Check out model summary.\n",
    "#model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para compilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc import callbacks\n",
    "import timeit\n",
    "from tracemalloc import start\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import json\n",
    "from keras.models import save_model\n",
    "epochs=100\n",
    "\n",
    "optimizer =SGD(lr=1e-2)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "            loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.8, patience=2,\n",
    "                              verbose=1, mode='max', min_lr=1e-10)\n",
    "\n",
    "callbacks=[reduce_lr]\n",
    "\n",
    "history=model.fit(trainDataset,epochs=epochs ,validation_data=testDataset,batch_size=batch_size,\n",
    "                                callbacks=callbacks,shuffle=True,\n",
    "                                #class_weight=class_weights,\n",
    "                                verbose=1)\n",
    "dir='/home/joshua6090/Documentos/DL_AECG_P/anomalias_en_electrocardiogramas_si/kunst/PosibleProp/pesos/'\n",
    "\n",
    "model.save('mmodel',save_format='keras')\n",
    "model.save('model/')\n",
    "#vamo a gusardar los pesos de esta madre.\n",
    "model.save_weights(filepath= 'pesos.h5')\n",
    "model.save(\"mi_modelo.h5\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curvas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(0)  \n",
    "plt.plot(history.history['accuracy'],'r')  \n",
    "plt.plot(history.history['val_accuracy'],'g')  \n",
    "plt.xticks(np.arange(0, 100, 10)) \n",
    "plt.yticks(np.arange(0, 1, 0.1))  \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Épocas\")  \n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.title(\"Accuracy del entramiento y validación\")  \n",
    "plt.legend(['Entrenamiento','Validación'])\n",
    "\n",
    "plt.figure(1)  \n",
    "plt.plot(history.history['loss'],'r')  \n",
    "plt.plot(history.history['val_loss'],'g')  \n",
    "plt.xticks(np.arange(0, 100, 10))\n",
    "   \n",
    "plt.rcParams['figure.figsize'] = (8, 6)  \n",
    "plt.xlabel(\"Épocas\")  \n",
    "plt.ylabel(\"Perdida\")  \n",
    "plt.title(\"Perdida del entramiento y validación\")  \n",
    "plt.legend(['Entrenamiento','Validación'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn import metrics\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "#####################################################################\n",
    "target_names=['N', 'Q','S','V','F']\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "#####################################################################\n",
    "errors=0\n",
    "y_pred=[]\n",
    "y_true=testDataset.labels # make sure shuffle=False in generator\n",
    "classes=list(testDataset.class_indices.keys())\n",
    "class_count=len(classes)\n",
    "preds=model.predict(testDataset, verbose=1)\n",
    "for i, p in enumerate(preds):        \n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=testDataset.labels[i]  # labels are integer values        \n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "        y_pred.append(pred_index)\n",
    "        \n",
    "tests=len(preds)\n",
    "acc=( 1-errors/tests) * 100\n",
    "msg=f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}'\n",
    "print(msg)\n",
    "#######################################################################\n",
    "ypred=np.array(y_pred)\n",
    "ytrue=np.array(y_true)\n",
    "##############################################################################\n",
    "cm = confusion_matrix(ytrue, ypred, normalize='true' )\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "plt.xticks(np.arange(class_count), classes, rotation=0)\n",
    "plt.yticks(np.arange(class_count), classes, rotation=0)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "##################################################################################\n",
    "clr = classification_report(y_true, y_pred, target_names=classes, digits= 3) # create classification report\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "##################################################################################\n",
    "#Bonita\n",
    "cm_display=ConfusionMatrixDisplay(cm,display_labels=['F','N', 'Q','S','V'])\n",
    "#print(cm)\n",
    "cm_display.plot(cmap='Blues') \n",
    "cm_display.ax_.set_title('Matriz de confusión')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true, y_pred, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_accuracies = {}\n",
    "for idx, cls in enumerate(classes):\n",
    "    true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "    true_positives = cm[idx, idx]\n",
    "    per_class_accuracies[cls] = (true_positives + true_negatives) / np.sum(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predxs=model.predict(testDataset)\n",
    "labelroc=testDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    all_labels = ['N', 'Q', 'S', 'V', 'F']\n",
    "    fig, c_ax = plt.subplots(1,1)\n",
    "    for (idx, c_label) in enumerate(all_labels):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.3f)' % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing',lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    c_ax.set_xlabel('Tasa de falsos positivos')\n",
    "    c_ax.set_ylabel('Tasa de verdaderos positivos')\n",
    "    c_ax.set_title('Característica Operativa del receptor')\n",
    "    c_ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "# calling\n",
    "testDataset.reset() # resetting generator\n",
    "y_pred = model.predict_generator(testDataset, verbose = True)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "multiclass_roc_auc_score(testDataset.classes, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
